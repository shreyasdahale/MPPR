{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fdede0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.linear_model import Ridge\n",
    "from matplotlib.colors import Normalize\n",
    "import networkx as nx\n",
    "from scipy.signal import welch\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from collections import defaultdict\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52baae13",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ecb4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lorenz_derivatives(state, t, sigma=10.0, rho=28.0, beta=8.0/3.0):\n",
    "#     \"\"\"Compute time derivatives [dx/dt, dy/dt, dz/dt] for the Lorenz system.\"\"\"\n",
    "#     x, y, z = state\n",
    "#     dxdt = sigma * (y - x)\n",
    "#     dydt = x * (rho - z) - y\n",
    "#     dzdt = x * y - beta * z\n",
    "#     return [dxdt, dydt, dzdt]\n",
    "\n",
    "# def generate_lorenz_data(\n",
    "#     initial_state=[1.0, 1.0, 1.0],\n",
    "#     tmax=25.0,\n",
    "#     dt=0.01,\n",
    "#     sigma=10.0,\n",
    "#     rho=28.0,\n",
    "#     beta=8.0/3.0\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Numerically integrate Lorenz equations x'(t), y'(t), z'(t) using odeint.\n",
    "#     Returns:\n",
    "#        t_vals: array of time points\n",
    "#        sol   : array shape [num_steps, 3] of [x(t), y(t), z(t)]\n",
    "#     \"\"\"\n",
    "#     num_steps = int(tmax / dt) + 1\n",
    "#     t_vals = np.linspace(0, tmax, num_steps)\n",
    "#     sol = odeint(lorenz_derivatives, initial_state, t_vals, args=(sigma, rho, beta))\n",
    "#     return t_vals, sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b88e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_system = \"Lorenz\"  # Options: \"lorenz\", \"rossler\", \"chen\", \"chua\"\n",
    "# tmax = 250.0\n",
    "# dt = 0.02\n",
    "# t_vals, lorenz_traj = generate_lorenz_data(\n",
    "#     initial_state=[1.0, 1.0, 1.0],\n",
    "#     tmax=tmax,\n",
    "#     dt=dt,\n",
    "#     sigma=10.0,\n",
    "#     rho=28.0,\n",
    "#     beta=8.0/3.0\n",
    "# )\n",
    "\n",
    "# # Discard first 2,000 points as washout\n",
    "# washout = 2000\n",
    "# t_vals = t_vals[washout:]\n",
    "# lorenz_trajectory = lorenz_traj[washout:]\n",
    "\n",
    "# T_data = len(lorenz_traj)\n",
    "# print(f\"Data length: {T_data}, from t=0..{tmax} with dt={dt}.\")\n",
    "\n",
    "# train_frac = 0.7\n",
    "# train_end = int(train_frac*(T_data-1))\n",
    "# # train_end = 4500\n",
    "# train_input  = lorenz_traj[:train_end]\n",
    "# train_target = lorenz_traj[1:train_end+1]\n",
    "# test_input   = lorenz_traj[train_end:-1]\n",
    "# test_target  = lorenz_traj[train_end+1:]\n",
    "\n",
    "# y_test = test_target\n",
    "# n_test_steps = len(test_target)\n",
    "# time_test = np.arange(n_test_steps)*dt\n",
    "# print(f\"Train size: {len(train_input)}  Test size: {len(test_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b9b6f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 6330\n"
     ]
    }
   ],
   "source": [
    "class LSTMBaseline3D:\n",
    "    \"\"\"\n",
    "    Lightweight single-layer LSTM for 3-dim Lorenz forecasting.\n",
    "    * hidden_size=32 → ~4.8k trainable parameters\n",
    "    * fit() trains in teacher-forcing mode\n",
    "    * predict() produces autoregressive roll-out\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim:  int = 3,\n",
    "                 hidden_size: int = 37,\n",
    "                 output_dim: int = 3,\n",
    "                 lr: float = 1e-3,\n",
    "                 epochs: int = 30,\n",
    "                 device: str = 'cpu',\n",
    "                 seed: int = 0):\n",
    "        torch.manual_seed(seed); np.random.seed(seed)\n",
    "\n",
    "        self.device  = torch.device(device)\n",
    "        self.epochs  = epochs\n",
    "        self.model   = nn.LSTM(input_dim, hidden_size,\n",
    "                               batch_first=True).to(self.device)\n",
    "        self.head    = nn.Linear(hidden_size, output_dim).to(self.device)\n",
    "        self.crit    = nn.MSELoss()\n",
    "        self.optim   = Adam(list(self.model.parameters())+\n",
    "                            list(self.head.parameters()), lr=lr)\n",
    "        \n",
    "    def total_parameters(self):\n",
    "        total = 0\n",
    "        for param in list(self.model.parameters()) + list(self.head.parameters()):\n",
    "            total += param.numel()\n",
    "        return total\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def _init_hidden(self, batch_sz=1):\n",
    "        h0 = torch.zeros(1, batch_sz,\n",
    "                         self.model.hidden_size,\n",
    "                         device=self.device)\n",
    "        c0 = torch.zeros_like(h0)\n",
    "        return (h0, c0)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    def fit(self, x_np: np.ndarray, y_np: np.ndarray):\n",
    "        \"\"\"\n",
    "        x_np shape [T, 3]  (input  at t)\n",
    "        y_np shape [T, 3]  (target at t)\n",
    "        \"\"\"\n",
    "        x = torch.tensor(x_np, dtype=torch.float32,\n",
    "                         device=self.device).unsqueeze(0)  # [1,T,3]\n",
    "        y = torch.tensor(y_np, dtype=torch.float32,\n",
    "                         device=self.device).unsqueeze(0)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            self.optim.zero_grad()\n",
    "            out, _ = self.model(x, self._init_hidden())\n",
    "            pred   = self.head(out)\n",
    "            loss   = self.crit(pred, y)\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def predict(self, init_u: np.ndarray, n_steps: int):\n",
    "        \"\"\"\n",
    "        Autoregressive roll-out.\n",
    "        init_u : initial 3-vector (last known sample)\n",
    "        Returns array of shape [n_steps, 3].\n",
    "        \"\"\"\n",
    "        self.model.eval(); self.head.eval()\n",
    "\n",
    "        inp     = torch.tensor(init_u[None, None, :],\n",
    "                               dtype=torch.float32, device=self.device)\n",
    "        h, c    = self._init_hidden()\n",
    "        preds   = np.empty((n_steps, 3), dtype=np.float32)\n",
    "\n",
    "        for t in range(n_steps):\n",
    "            out, (h, c) = self.model(inp, (h, c))\n",
    "            y           = self.head(out)\n",
    "            preds[t]    = y.squeeze(0).cpu().numpy()\n",
    "            inp         = y.detach()    # feed prediction back\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_open_loop(self, x_np: np.ndarray):\n",
    "        \"\"\"\n",
    "        Open-loop prediction using teacher-forced inputs (like during training).\n",
    "        x_np shape: [T, 3] – input sequence\n",
    "        Returns:\n",
    "            preds: [T, 3] – predicted output sequence\n",
    "        \"\"\"\n",
    "        self.model.eval(); self.head.eval()\n",
    "\n",
    "        x = torch.tensor(x_np, dtype=torch.float32,\n",
    "                         device=self.device).unsqueeze(0)  # [1, T, 3]\n",
    "        out, _ = self.model(x, self._init_hidden())\n",
    "        preds = self.head(out).squeeze(0).cpu().numpy()  # [T, 3]\n",
    "\n",
    "        return preds\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = LSTMBaseline3D()\n",
    "    print(f\"Total trainable parameters: {model.total_parameters()}\")\n",
    "\n",
    "# lstm_baseline = LSTMBaseline3D(\n",
    "#                     hidden_size=38,         # parameter budget ~ 4800\n",
    "#                     lr=1e-3,\n",
    "#                     epochs=100,\n",
    "#                     device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "#                     seed=45)\n",
    "# lstm_baseline.fit(train_input, train_target)\n",
    "\n",
    "# # one-step roll-out to build an initial vector for auto-regressive mode\n",
    "# init_vec = train_target[-1]                # last teacher-forced target\n",
    "# lstm_preds = lstm_baseline.predict(init_vec,\n",
    "#                                    n_steps=len(test_input))\n",
    "# lstm_preds_open_loop = lstm_baseline.predict_open_loop(test_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011886ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNBaseline3D(nn.Module):\n",
    "    \"\"\"\n",
    "    2-layer causal TCN       (kernel=3, dilation=1 & 2, padding chosen\n",
    "    so receptive field = 5 time-steps, identical to NVAR window length).\n",
    "    ----------------------\n",
    "    • input_dim  = 3\n",
    "    • hidden_dim = 32  → total ≈ 4.9 k parameters\n",
    "    • output_dim = 3    (one-step prediction)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_dim:  int = 3,\n",
    "                 hidden_dim: int = 32,\n",
    "                 output_dim: int = 3,\n",
    "                 lr: float = 1e-3,\n",
    "                 epochs: int = 40,\n",
    "                 device: str = \"cpu\",\n",
    "                 seed: int = 0):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed); np.random.seed(seed)\n",
    "\n",
    "        k = 3  # kernel\n",
    "        # layer 1: dilation 1  → pad 2 to keep length\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim,\n",
    "                               kernel_size=k,\n",
    "                               dilation=1,\n",
    "                               padding=2,\n",
    "                               bias=True)\n",
    "        # layer 2: dilation 2  → pad 4\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim,\n",
    "                               kernel_size=k,\n",
    "                               dilation=2,\n",
    "                               padding=4,\n",
    "                               bias=True)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.head  = nn.Conv1d(hidden_dim, output_dim,\n",
    "                               kernel_size=1, bias=True)\n",
    "\n",
    "        self.lr, self.epochs = lr, epochs\n",
    "        self.to(device)\n",
    "        self.optim = Adam(self.parameters(), lr=lr)\n",
    "        self.crit  = nn.MSELoss()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape  [B, T, 3]  (batch, time, channels)\n",
    "        return  [B, T, 3]\n",
    "        \"\"\"\n",
    "        # reshape to Conv1d convention: (B, C, T)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        y = self.conv1(x); y = self.relu(y[:, :, :-2])     # remove look-ahead pad\n",
    "        y = self.conv2(y); y = self.relu(y[:, :, :-4])     # remove look-ahead pad\n",
    "        out = self.head(y).permute(0, 2, 1)                # back to (B,T,C)\n",
    "        return out\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    def fit(self, x_np: np.ndarray, y_np: np.ndarray):\n",
    "        \"\"\"\n",
    "        Teacher-forcing on entire sequence (batch size = 1).\n",
    "        x_np, y_np shape [T, 3]\n",
    "        \"\"\"\n",
    "        x = torch.tensor(x_np[None], dtype=torch.float32, device=next(self.parameters()).device)\n",
    "        y = torch.tensor(y_np[None], dtype=torch.float32, device=next(self.parameters()).device)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            self.optim.zero_grad()\n",
    "            pred = self.forward(x)\n",
    "            loss = self.crit(pred[:, :-1], y[:, 1:])  # predict next step\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def predict(self, init_window: np.ndarray, n_steps: int):\n",
    "        \"\"\"\n",
    "        Autoregressive roll-out.\n",
    "        init_window : length L≥5, shape [L,3] (latest samples, earliest first)\n",
    "        Returns      : [n_steps,3]\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        window = init_window.copy()\n",
    "        preds  = np.empty((n_steps, 3), dtype=np.float32)\n",
    "\n",
    "        for t in range(n_steps):\n",
    "            inp = torch.tensor(window[None], dtype=torch.float32, device=device)\n",
    "            y   = self.forward(inp)[0, -1].cpu().numpy()\n",
    "            preds[t] = y\n",
    "            window   = np.vstack([window[1:], y])  # slide window\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "# tcn = TCNBaseline3D(hidden_dim=32, epochs=50, lr=1e-3, device=\"cpu\", seed=46)\n",
    "# tcn.fit(train_input, train_target)\n",
    "\n",
    "# # initial window must be ≥5 samples:\n",
    "# init_win = test_input[:5].copy()\n",
    "# tcn_preds = tcn.predict(init_win, n_steps=len(test_target))\n",
    "\n",
    "# ============================================================\n",
    "#  Causal Transformer baseline for 3-D Lorenz forecasting\n",
    "#  (PyTorch ≥ 1.9)\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "class SmallCausalTransformer3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Single-layer causal Transformer:\n",
    "      • d_model = 24,   nhead = 1,   d_ff = 4·d_model\n",
    "      • receptive field  = sequence length L (set in fit / predict)\n",
    "      • total parameters ≈ 4 900\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_model: int = 24,\n",
    "                 nhead: int = 1,\n",
    "                 d_ff: int = 96,        # 4 × d_model\n",
    "                 lr: float = 2e-3,\n",
    "                 epochs: int = 60,\n",
    "                 device: str = \"cpu\",\n",
    "                 seed: int = 0):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed); np.random.seed(seed)\n",
    "        self.device, self.epochs = device, epochs\n",
    "\n",
    "        self.in_proj   = nn.Linear(3, d_model)     # 3-dim input → tokens\n",
    "        encoder_layer  = nn.TransformerEncoderLayer(\n",
    "                             d_model=d_model,\n",
    "                             nhead=nhead,\n",
    "                             dim_feedforward=d_ff,\n",
    "                             batch_first=True,\n",
    "                             activation=\"gelu\",\n",
    "                             norm_first=True)\n",
    "        self.encoder   = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.pos_embed = None                      # built on first call\n",
    "        self.head      = nn.Linear(d_model, 3)     # back to 3-dim output\n",
    "\n",
    "        self.to(device)\n",
    "        self.opt  = Adam(self.parameters(), lr=lr)\n",
    "        self.crit = nn.MSELoss()\n",
    "\n",
    "    # ----------------------------------------\n",
    "    def _get_posembed(self, L: int, d: int):\n",
    "        \"\"\"Fixed sinusoidal positional embedding (same as Vaswani et al.).\"\"\"\n",
    "        pos = torch.arange(L, dtype=torch.float32, device=self.device)\n",
    "        i   = torch.arange(d//2, dtype=torch.float32, device=self.device)\n",
    "        angles = pos[:, None] / (10000 ** (2*i/d))\n",
    "        pe = torch.zeros(L, d, device=self.device)\n",
    "        pe[:, 0::2] = torch.sin(angles)\n",
    "        pe[:, 1::2] = torch.cos(angles)\n",
    "        return pe[None]                                # shape (1,L,d)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    def fit(self, x_np: np.ndarray, y_np: np.ndarray, L: int = 20):\n",
    "        \"\"\"\n",
    "        Teacher-forcing with sliding windows of length L.\n",
    "        x_np, y_np  shape [T, 3];  y_np[t] is the desired prediction for x_np[t].\n",
    "        \"\"\"\n",
    "        x = torch.tensor(x_np, dtype=torch.float32, device=self.device)\n",
    "        y = torch.tensor(y_np, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        if self.pos_embed is None or self.pos_embed.size(1) != L:\n",
    "            self.pos_embed = self._get_posembed(L, self.in_proj.out_features)\n",
    "\n",
    "        # build training batches as overlapping windows (stride 1)\n",
    "        windows   = x.unfold(0, L, 1)        # shape [T-L+1, L, 3]\n",
    "        targets   = y[L-1:]                  # predict the last step\n",
    "        dataset   = torch.utils.data.TensorDataset(windows, targets)\n",
    "        loader    = torch.utils.data.DataLoader(dataset,\n",
    "                                                batch_size=64,\n",
    "                                                shuffle=True)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for batch_x, batch_y in loader:\n",
    "                self.opt.zero_grad()\n",
    "                z   = self.in_proj(batch_x) + self.pos_embed\n",
    "                out = self.encoder(z)\n",
    "                pred = self.head(out[:, -1])          # last token\n",
    "                loss = self.crit(pred, batch_y)\n",
    "                loss.backward(); self.opt.step()\n",
    "\n",
    "    # ----------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def predict(self, init_window: np.ndarray, n_steps: int):\n",
    "        \"\"\"\n",
    "        Autoregressive roll-out.\n",
    "        init_window : numpy (L,3)  – most recent L samples (old → new)\n",
    "        Returns      : numpy (n_steps,3)\n",
    "        \"\"\"\n",
    "        L = init_window.shape[0]\n",
    "        if self.pos_embed is None or self.pos_embed.size(1) != L:\n",
    "            self.pos_embed = self._get_posembed(L, self.in_proj.out_features)\n",
    "\n",
    "        window = torch.tensor(init_window, dtype=torch.float32,\n",
    "                              device=self.device)\n",
    "        preds  = np.empty((n_steps, 3), dtype=np.float32)\n",
    "\n",
    "        for t in range(n_steps):\n",
    "            z   = self.in_proj(window[None]) + self.pos_embed\n",
    "            y   = self.head(self.encoder(z)[:, -1])[0]\n",
    "            preds[t] = y.cpu().numpy()\n",
    "\n",
    "            window = torch.vstack([window[1:], y])\n",
    "\n",
    "        return preds\n",
    "\n",
    "# # ---------------------------------------------------\n",
    "# #  hyper-parameters chosen to match ≈ 5 k weights\n",
    "# # ---------------------------------------------------\n",
    "# tformer = SmallCausalTransformer3D(d_model=24,\n",
    "#                                    d_ff=96,\n",
    "#                                    epochs=60,\n",
    "#                                    lr=2e-3,\n",
    "#                                    device=\"cpu\",\n",
    "#                                    seed=47)\n",
    "\n",
    "# seq_len = 20                         # receptive field (same as NVAR window)\n",
    "# tformer.fit(train_input, train_target, L=seq_len)\n",
    "\n",
    "# init_win = test_input[:seq_len].copy()\n",
    "# tf_preds = tformer.predict(init_win, n_steps=len(test_target))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  Non-linear Vector Auto-Regression (NVAR) baseline for 3-D Lorenz\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "from itertools import combinations_with_replacement\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "class NVARBaseline3D:\n",
    "    \"\"\"\n",
    "    • delay window length k   (default 5 samples)\n",
    "    • quadratic polynomial lift (all monomials up to degree 2)\n",
    "    • closed-form ridge regression read-out\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 k: int = 5,\n",
    "                 ridge_alpha: float = 1e-4):\n",
    "        self.k          = k\n",
    "        self.alpha      = ridge_alpha\n",
    "        self.scaler_mu  = None\n",
    "        self.scaler_sig = None\n",
    "        self.reg        = Ridge(alpha=self.alpha, fit_intercept=False)\n",
    "\n",
    "        # indices for quadratic terms\n",
    "        L  = 3 * k                 # length of flattened delay vector\n",
    "        self.idxs_quad = list(combinations_with_replacement(range(L), 2))\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    def _build_feature(self, window: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        window: shape (k, 3)  -> returns (F,) where\n",
    "          F = 1 + 3k + (3k)(3k+1)/2\n",
    "        \"\"\"\n",
    "        lin = window.flatten()                 # linear terms\n",
    "        quad = np.array([lin[i]*lin[j] for i, j in self.idxs_quad])\n",
    "        return np.concatenate(([1.0], lin, quad), dtype=np.float32)\n",
    "    \n",
    "    def total_parameters(self):\n",
    "        total = 0\n",
    "        for param in list(self.model.parameters()) + list(self.head.parameters()):\n",
    "            total += param.numel()\n",
    "        return total\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    def fit(self, x_np: np.ndarray, y_np: np.ndarray):\n",
    "        \"\"\"\n",
    "        x_np shape [T, 3] (driver)\n",
    "        y_np shape [T, 3] (target 1-step ahead)\n",
    "        Assumes x_np[t] predicts y_np[t].\n",
    "        \"\"\"\n",
    "        k = self.k\n",
    "        assert len(x_np) == len(y_np)\n",
    "        # normalise inputs\n",
    "        self.scaler_mu  = x_np.mean(0, keepdims=True)\n",
    "        self.scaler_sig = x_np.std (0, keepdims=True) + 1e-9\n",
    "        x_norm = (x_np - self.scaler_mu)/self.scaler_sig\n",
    "\n",
    "        feats, targets = [], []\n",
    "        for t in range(k, len(x_norm)):\n",
    "            window = x_norm[t-k:t]              # shape (k,3)\n",
    "            feats.append(self._build_feature(window))\n",
    "            targets.append(y_np[t])\n",
    "\n",
    "        X = np.vstack(feats)\n",
    "        Y = np.vstack(targets)\n",
    "        self.reg.fit(X, Y)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    def predict(self, init_window: np.ndarray, n_steps: int):\n",
    "        \"\"\"\n",
    "        Autoregressive roll-out.\n",
    "        init_window : array (k,3)  – most recent k inputs (y-values).\n",
    "        Returns array (n_steps,3)\n",
    "        \"\"\"\n",
    "        k = self.k\n",
    "        window = init_window.copy()\n",
    "        preds  = np.empty((n_steps, 3), dtype=np.float32)\n",
    "\n",
    "        for t in range(n_steps):\n",
    "            w_norm  = (window - self.scaler_mu)/self.scaler_sig\n",
    "            phi     = self._build_feature(w_norm)\n",
    "            y_hat   = self.reg.predict(phi[None, :])[0]\n",
    "            preds[t] = y_hat\n",
    "            # slide window: drop oldest, append new prediction\n",
    "            window = np.vstack([window[1:], y_hat])\n",
    "\n",
    "        return preds\n",
    "\n",
    "def predict_open_loop(self, input_sequence: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Open-loop (teacher-forced) prediction using true inputs.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_sequence: np.ndarray of shape (T, 3), full sequence of inputs.\n",
    "    \n",
    "    Returns:\n",
    "    - preds: np.ndarray of shape (T - k, 3), predicted outputs using true inputs.\n",
    "    \"\"\"\n",
    "    k = self.k\n",
    "    x_norm = (input_sequence - self.scaler_mu) / self.scaler_sig\n",
    "    preds = []\n",
    "\n",
    "    for t in range(k, len(x_norm)):\n",
    "        window = x_norm[t-k:t]  # shape (k, 3)\n",
    "        phi = self._build_feature(window)\n",
    "        y_hat = self.reg.predict(phi[None, :])[0]\n",
    "        preds.append(y_hat)\n",
    "\n",
    "    return np.vstack(preds)\n",
    "\n",
    "# # ---------------------------------------------------\n",
    "# #  create training windows  (same split as before)\n",
    "# # ---------------------------------------------------\n",
    "# k = 5\n",
    "# nvar = NVARBaseline3D(k=k, ridge_alpha=1e-4)\n",
    "# nvar.fit(train_input, train_target)\n",
    "\n",
    "# # prepare the last k samples as initial window\n",
    "# init_window = test_input[:k].copy()\n",
    "# nvar_preds  = nvar.predict(init_window, n_steps=len(test_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "878d0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nrmse(all_preds, test_target, horizons):\n",
    "    \"\"\"\n",
    "    Evaluate model performance over multiple prediction horizons\n",
    "    for teacher-forced single-step forecasting or autoregressive rollout.\n",
    "    \"\"\"\n",
    "    horizon_nrmse = {}\n",
    "    for horizon in horizons:\n",
    "        preds = all_preds[:horizon]\n",
    "        targets = test_target[:horizon]\n",
    "        squared_errors = (preds - targets) ** 2\n",
    "        variance = np.var(targets, axis=0)\n",
    "        variance[variance == 0] = 1e-8  # avoid divide-by-zero\n",
    "        nrmse = np.sqrt(np.sum(squared_errors) / (horizon * np.sum(variance)))\n",
    "        horizon_nrmse[horizon] = nrmse\n",
    "    return horizon_nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96a6add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_valid_prediction_time(y_true, y_pred, t_vals, threshold, lambda_max, dt):\n",
    "    \"\"\"\n",
    "    Compute the Valid Prediction Time (VPT) and compare it to Lyapunov time T_lambda = 1 / lambda_max.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : ndarray of shape (N, dim)\n",
    "        True trajectory over time.\n",
    "    y_pred : ndarray of shape (N, dim)\n",
    "        Model's predicted trajectory over time (closed-loop).\n",
    "    t_vals : ndarray of shape (N,)\n",
    "        Time values corresponding to the trajectory steps.\n",
    "    threshold : float, optional\n",
    "        The error threshold, default is 0.4 as in your snippet.\n",
    "    lambda_max : float, optional\n",
    "        Largest Lyapunov exponent. Default=0.9 for Lorenz.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    T_VPT : float\n",
    "        Valid prediction time. The earliest time at which normalized error surpasses threshold\n",
    "        (or the last time if never surpassed).\n",
    "    T_lambda : float\n",
    "        Lyapunov time = 1 / lambda_max\n",
    "    ratio : float\n",
    "        How many Lyapunov times the model prediction remains valid, i.e. T_VPT / T_lambda.\n",
    "    \"\"\"\n",
    "    # 1) Average of y_true\n",
    "    y_mean = np.mean(y_true, axis=0)  # shape (dim,)\n",
    "    \n",
    "    # 2) Time-averaged norm^2 of (y_true - y_mean)\n",
    "    y_centered = y_true - y_mean\n",
    "    denom = np.mean(np.sum(y_centered**2, axis=1))  # scalar\n",
    "    \n",
    "    # 3) Compute the normalized error delta_gamma(t) = ||y_true - y_pred||^2 / denom\n",
    "    diff = y_true - y_pred\n",
    "    err_sq = np.sum(diff**2, axis=1)  # shape (N,)\n",
    "    delta_gamma = err_sq / denom      # shape (N,)\n",
    "    \n",
    "    # 4) Find the first time index where delta_gamma(t) exceeds threshold\n",
    "    idx_exceed = np.where(delta_gamma > threshold)[0]\n",
    "    if len(idx_exceed) == 0:\n",
    "        # never exceeds threshold => set T_VPT to the final time\n",
    "        T_VPT = t_vals[-1]\n",
    "    else:\n",
    "        T_VPT = t_vals[idx_exceed[0]]\n",
    "    \n",
    "    # 5) Compute T_lambda and ratio\n",
    "    T_lambda = 1.0 / lambda_max\n",
    "\n",
    "    # print(f\"\\n--- Valid Prediction Time (VPT) with threshold={threshold}, lambda_max={lambda_max} ---\")\n",
    "\n",
    "    T_VPT = (T_VPT - t_vals[0])  # Adjust T_VPT to be relative to the start time\n",
    "    ratio = T_VPT / T_lambda\n",
    "\n",
    "    return T_VPT, T_lambda, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b5c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attractor_deviation(predictions, targets, cube_size=(0.1, 0.1, 0.1)):\n",
    "    \"\"\"\n",
    "    Compute the Attractor Deviation (ADev) metric.\n",
    "\n",
    "    Parameters:\n",
    "        predictions (numpy.ndarray): Predicted trajectories of shape (n, 3).\n",
    "        targets (numpy.ndarray): True trajectories of shape (n, 3).\n",
    "        cube_size (tuple): Dimensions of the cube (dx, dy, dz).\n",
    "\n",
    "    Returns:\n",
    "        float: The ADev metric.\n",
    "    \"\"\"\n",
    "    # Define the cube grid based on the range of the data and cube size\n",
    "    min_coords = np.min(np.vstack((predictions, targets)), axis=0)\n",
    "    max_coords = np.max(np.vstack((predictions, targets)), axis=0)\n",
    "\n",
    "    # Create a grid of cubes\n",
    "    grid_shape = ((max_coords - min_coords) / cube_size).astype(int) + 1\n",
    "\n",
    "    # Initialize the cube occupancy arrays\n",
    "    pred_cubes = np.zeros(grid_shape, dtype=int)\n",
    "    target_cubes = np.zeros(grid_shape, dtype=int)\n",
    "\n",
    "    # Map trajectories to cubes\n",
    "    pred_indices = ((predictions - min_coords) / cube_size).astype(int)\n",
    "    target_indices = ((targets - min_coords) / cube_size).astype(int)\n",
    "\n",
    "    # Mark cubes visited by predictions and targets\n",
    "    for idx in pred_indices:\n",
    "        pred_cubes[tuple(idx)] = 1\n",
    "    for idx in target_indices:\n",
    "        target_cubes[tuple(idx)] = 1\n",
    "\n",
    "    # Compute the ADev metric\n",
    "    adev = np.sum(np.abs(pred_cubes - target_cubes))\n",
    "    return adev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00486cc",
   "metadata": {},
   "source": [
    "### MIT-BIH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826976d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 25000, train size: 15000, test size: 9999\n"
     ]
    }
   ],
   "source": [
    "# def create_delay_embedding(signal, embed_dim):\n",
    "#     L = len(signal) - embed_dim + 1\n",
    "#     emb = np.zeros((L, embed_dim))\n",
    "#     for i in range(L):\n",
    "#         emb[i, :] = signal[i:i+embed_dim]\n",
    "#     return emb\n",
    "# import wfdb\n",
    "\n",
    "# # Download and load record and annotations for patient #100\n",
    "# record = wfdb.rdrecord('100', sampfrom=0, sampto=25002, pn_dir='mitdb')  # first 20,000 samples\n",
    "# annotation = wfdb.rdann('100', 'atr', sampfrom=0, sampto=25002, pn_dir='mitdb')\n",
    "# # Get input signal u(t) from the first channel\n",
    "# u = record.p_signal[:, 0] \n",
    "# u\n",
    "# # Normalize input\n",
    "# u_min = np.min(u)\n",
    "# u_max = np.max(u)\n",
    "# u_norm = (u - u_min) / (u_max - u_min)\n",
    "# fs = record.fs  # sampling frequency (should be 360 Hz)\n",
    "# t_vals = np.arange(len(u_norm)) / fs\n",
    "# emb_dim = 3\n",
    "# # inputs = u_norm\n",
    "# inputs = create_delay_embedding(u_norm, emb_dim)\n",
    "\n",
    "# # Create target array (heartbeat locations)\n",
    "# targets = np.zeros(len(u_norm))\n",
    "# targets[annotation.sample] = 1  # mark annotations as 1 (heartbeat)\n",
    "# targets = create_delay_embedding(targets, emb_dim)\n",
    "# data_size = len(inputs)\n",
    "# train_size = 15000\n",
    "# train_input = inputs[:train_size]\n",
    "# train_target = targets[:train_size]\n",
    "# test_input = inputs[train_size+1:]\n",
    "# test_target = targets[train_size+1:]\n",
    "# test_size = len(test_input)\n",
    "# print(f\"Total samples: {data_size}, train size: {train_size}, test size: {test_size}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feca1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baaadf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011503\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      7\u001b[39m lstm_baseline = LSTMBaseline3D(\n\u001b[32m      8\u001b[39m                     hidden_size=\u001b[32m500\u001b[39m,         \u001b[38;5;66;03m# parameter budget ~ 4800\u001b[39;00m\n\u001b[32m      9\u001b[39m                     lr=\u001b[32m1e-3\u001b[39m,\n\u001b[32m     10\u001b[39m                     epochs=\u001b[32m80\u001b[39m,\n\u001b[32m     11\u001b[39m                     device=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     12\u001b[39m                     seed=seed)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(lstm_baseline.total_parameters())\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mlstm_baseline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# one-step roll-out to build an initial vector for auto-regressive mode\u001b[39;00m\n\u001b[32m     17\u001b[39m init_vec = train_target[-\u001b[32m1\u001b[39m]                \u001b[38;5;66;03m# last teacher-forced target\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mLSTMBaseline3D.fit\u001b[39m\u001b[34m(self, x_np, y_np)\u001b[39m\n\u001b[32m     57\u001b[39m pred   = \u001b[38;5;28mself\u001b[39m.head(out)\n\u001b[32m     58\u001b[39m loss   = \u001b[38;5;28mself\u001b[39m.crit(pred, y)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m.optim.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# all_horizons = list(range(10, 1001, 10))\n",
    "\n",
    "# nrmse_dict = defaultdict(list)\n",
    "# seeds = range(995, 996)\n",
    "\n",
    "# for seed in seeds:\n",
    "#     lstm_baseline = LSTMBaseline3D(\n",
    "#                         hidden_size=500,         # parameter budget ~ 4800\n",
    "#                         lr=1e-3,\n",
    "#                         epochs=80,\n",
    "#                         device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "#                         seed=seed)\n",
    "#     print(lstm_baseline.total_parameters())\n",
    "#     lstm_baseline.fit(train_input, train_target)\n",
    "\n",
    "#     # one-step roll-out to build an initial vector for auto-regressive mode\n",
    "#     init_vec = train_target[-1]                # last teacher-forced target\n",
    "#     lstm_preds = lstm_baseline.predict_open_loop(test_input)\n",
    "\n",
    "#     nrmse = evaluate_nrmse(lstm_preds, test_target, all_horizons)\n",
    "#     nrmse_dict['LSTM'].append(nrmse)\n",
    "#     # for horizon, value in nrmse.items():\n",
    "#     #     nrmse_dict[horizon].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244cba47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'LSTM': [{10: np.float64(0.8519464187323911),\n",
       "               20: np.float64(0.7958150288342488),\n",
       "               30: np.float64(0.7892290152445449),\n",
       "               40: np.float64(0.7862312593649706),\n",
       "               50: np.float64(0.7845580402223172),\n",
       "               60: np.float64(0.783669667836264),\n",
       "               70: np.float64(0.7831128224267435),\n",
       "               80: np.float64(0.7826650143148804),\n",
       "               90: np.float64(0.7825183672406559),\n",
       "               100: np.float64(0.7825381023000931),\n",
       "               110: np.float64(0.7826550225468832),\n",
       "               120: np.float64(0.7826047477720924),\n",
       "               130: np.float64(0.7824413141975098),\n",
       "               140: np.float64(0.7825035859779347),\n",
       "               150: np.float64(0.7824981078251767),\n",
       "               160: np.float64(0.7825431863408008),\n",
       "               170: np.float64(0.7825674542505371),\n",
       "               180: np.float64(0.782522152892447),\n",
       "               190: np.float64(0.7824632997557308),\n",
       "               200: np.float64(0.7823875151389382),\n",
       "               210: np.float64(0.7823428991451182),\n",
       "               220: np.float64(0.78234489814936),\n",
       "               230: np.float64(0.7824918987031206),\n",
       "               240: np.float64(0.7831373476365813),\n",
       "               250: np.float64(0.7842933943678109),\n",
       "               260: np.float64(0.7852793217230465),\n",
       "               270: np.float64(0.7855595213866922),\n",
       "               280: np.float64(0.7855269267584001),\n",
       "               290: np.float64(0.7855081639908614),\n",
       "               300: np.float64(0.7874808530534102),\n",
       "               310: np.float64(0.7753069186327466),\n",
       "               320: np.float64(0.7816266170855377),\n",
       "               330: np.float64(0.7816051686891584),\n",
       "               340: np.float64(0.7816044381274816),\n",
       "               350: np.float64(0.7816027946800517),\n",
       "               360: np.float64(0.7815915080302224),\n",
       "               370: np.float64(0.7816021970687999),\n",
       "               380: np.float64(0.7815996040431203),\n",
       "               390: np.float64(0.7815933947184747),\n",
       "               400: np.float64(0.7816330015668553),\n",
       "               410: np.float64(0.7816730462993148),\n",
       "               420: np.float64(0.781663138899907),\n",
       "               430: np.float64(0.7816981300676248),\n",
       "               440: np.float64(0.7818407512979133),\n",
       "               450: np.float64(0.7819520444005336),\n",
       "               460: np.float64(0.7819937031706665),\n",
       "               470: np.float64(0.7820185312517527),\n",
       "               480: np.float64(0.7820308592423829),\n",
       "               490: np.float64(0.7820162224360968),\n",
       "               500: np.float64(0.782014595137812),\n",
       "               510: np.float64(0.7820001493764924),\n",
       "               520: np.float64(0.7820016592300758),\n",
       "               530: np.float64(0.7821696199502308),\n",
       "               540: np.float64(0.7827487737706167),\n",
       "               550: np.float64(0.7832473741876701),\n",
       "               560: np.float64(0.7839562077231433),\n",
       "               570: np.float64(0.7839558870749435),\n",
       "               580: np.float64(0.7839483816092637),\n",
       "               590: np.float64(0.7839483654213951),\n",
       "               600: np.float64(0.7849500085960324),\n",
       "               610: np.float64(0.7822240698392626),\n",
       "               620: np.float64(0.7832276483815603),\n",
       "               630: np.float64(0.7832534861134308),\n",
       "               640: np.float64(0.7833046798059641),\n",
       "               650: np.float64(0.7833603948802972),\n",
       "               660: np.float64(0.7834056763904768),\n",
       "               670: np.float64(0.7834904091514822),\n",
       "               680: np.float64(0.7835319263928908),\n",
       "               690: np.float64(0.783554472915925),\n",
       "               700: np.float64(0.7836307961077066),\n",
       "               710: np.float64(0.783772468030192),\n",
       "               720: np.float64(0.7838388227400834),\n",
       "               730: np.float64(0.783840894014438),\n",
       "               740: np.float64(0.7838766176671699),\n",
       "               750: np.float64(0.7838886016615821),\n",
       "               760: np.float64(0.7838882091873228),\n",
       "               770: np.float64(0.7838821326645151),\n",
       "               780: np.float64(0.7838692903798323),\n",
       "               790: np.float64(0.7838568391849997),\n",
       "               800: np.float64(0.7838559851509665),\n",
       "               810: np.float64(0.7838452319697551),\n",
       "               820: np.float64(0.7838349922011542),\n",
       "               830: np.float64(0.7839583048081817),\n",
       "               840: np.float64(0.7842471112675905),\n",
       "               850: np.float64(0.7843913049371042),\n",
       "               860: np.float64(0.7844248352327093),\n",
       "               870: np.float64(0.7844283730038156),\n",
       "               880: np.float64(0.7844361030813892),\n",
       "               890: np.float64(0.784955656075178),\n",
       "               900: np.float64(0.788115784216893),\n",
       "               910: np.float64(0.7893981114443589),\n",
       "               920: np.float64(0.7894208596165203),\n",
       "               930: np.float64(0.7894724603370302),\n",
       "               940: np.float64(0.7895488329356586),\n",
       "               950: np.float64(0.7896049380281136),\n",
       "               960: np.float64(0.7896883562363962),\n",
       "               970: np.float64(0.7897568336812971),\n",
       "               980: np.float64(0.7898320936466308),\n",
       "               990: np.float64(0.7900577151541113),\n",
       "               1000: np.float64(0.7902527765125783)}]})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nrmse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd37e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('lstmmitbih.json', 'w') as f:\n",
    "#     json.dump(nrmse_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd294f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRMSE for Different Prediction Horizons:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "LSTM             \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "300        0.7874808530534102 ± 0.0\n",
      "600        0.7849500085960324 ± 0.0\n",
      "1000       0.7902527765125783 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "# horizons = [300, 600, 1000]\n",
    "# print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "# print(\"-\" * 140)\n",
    "# print(f\"{'LSTM':<17}\")\n",
    "# print(\"-\" * 140)\n",
    "\n",
    "# for horizon in horizons:\n",
    "#     hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['LSTM']]\n",
    "\n",
    "#     print(f\"{horizon:<10}\", end=\" \")\n",
    "#     for vals in [hfr_vals]:\n",
    "#         mean = np.mean(vals)\n",
    "#         std = np.std(vals)\n",
    "#         print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b5ba0",
   "metadata": {},
   "source": [
    "### Sunspot (Monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7161afa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3315\n",
      "Train size: 2000\n",
      "Test size: 1312\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# file_path = '../RealWorld/datasets/SN_m_tot_V2.0.csv'\n",
    "\n",
    "# df = pd.read_csv(file_path, sep=';', header = None)\n",
    "# df\n",
    "# data = df.iloc[:, 3].values\n",
    "# dt = 1\n",
    "# dataset_size = len(data)\n",
    "# data = create_delay_embedding(data, 3)\n",
    "# print(f\"Dataset size: {dataset_size}\")\n",
    "\n",
    "# # Train/Test Split\n",
    "# train_end = 2000\n",
    "# train_input  = data[:train_end]\n",
    "# train_target = data[1:train_end+1]\n",
    "# test_input   = data[train_end:-1]\n",
    "# test_target  = data[train_end+1:]\n",
    "# y_test = test_target\n",
    "# n_test_steps = len(test_target)\n",
    "# time_test = np.arange(n_test_steps) * dt\n",
    "\n",
    "# print(f\"Train size: {len(train_input)}\\nTest size: {len(test_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931fe915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_baseline = LSTMBaseline3D(\n",
    "#                     hidden_size=32,         # parameter budget ~ 4800\n",
    "#                     lr=1e-3,\n",
    "#                     epochs=400,\n",
    "#                     device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "#                     seed=42)\n",
    "# lstm_baseline.fit(train_input, train_target)\n",
    "\n",
    "# # one-step roll-out to build an initial vector for auto-regressive mode\n",
    "# init_vec = train_target[-1]                # last teacher-forced target\n",
    "# lstm_preds = lstm_baseline.predict_open_loop(test_input)\n",
    "\n",
    "# nrmse = evaluate_nrmse(lstm_preds, test_target, horizons=[200, 400, 600, 800, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb23aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5dc631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4023003\n"
     ]
    }
   ],
   "source": [
    "# horizons = [300, 600, 1000]\n",
    "\n",
    "# all_horizons = list(range(10, 1001, 10))\n",
    "\n",
    "# nrmse_dict = defaultdict(list)\n",
    "# seeds = range(995, 996)\n",
    "\n",
    "# for seed in seeds:\n",
    "#     lstm_baseline = LSTMBaseline3D(\n",
    "#                         hidden_size=1000,         # parameter budget ~ 4800\n",
    "#                         lr=1e-3,\n",
    "#                         epochs=200,\n",
    "#                         device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "#                         seed=seed)\n",
    "#     print(lstm_baseline.total_parameters())\n",
    "#     lstm_baseline.fit(train_input, train_target)\n",
    "\n",
    "#     # one-step roll-out to build an initial vector for auto-regressive mode\n",
    "#     init_vec = train_target[-1]                # last teacher-forced target\n",
    "#     lstm_preds = lstm_baseline.predict_open_loop(test_input)\n",
    "\n",
    "#     nrmse = evaluate_nrmse(lstm_preds, test_target, all_horizons)\n",
    "#     nrmse_dict['LSTM'].append(nrmse)\n",
    "#     # for horizon, value in nrmse.items():\n",
    "#     #     nrmse_dict[horizon].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db726d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRMSE for Different Prediction Horizons:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "LSTM             \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "300        0.5405236588497067 ± 0.0\n",
      "600        0.6684337897573301 ± 0.0\n",
      "1000       0.6586615560957684 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "# print(\"-\" * 140)\n",
    "# print(f\"{'LSTM':<17}\")\n",
    "# print(\"-\" * 140)\n",
    "\n",
    "# for horizon in horizons:\n",
    "#     hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['LSTM']]\n",
    "\n",
    "#     print(f\"{horizon:<10}\", end=\" \")\n",
    "#     for vals in [hfr_vals]:\n",
    "#         mean = np.mean(vals)\n",
    "#         std = np.std(vals)\n",
    "#         print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad54a6",
   "metadata": {},
   "source": [
    "### Santa Fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15ec1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 17000.\n",
      "Train size: 7000  \n",
      "Test size: 9997\n"
     ]
    }
   ],
   "source": [
    "# file_path = 'RealWorld/datasets/santa-fe-time-series-competition-data-set-b-1.0.0/b1.txt'\n",
    "\n",
    "# df = pd.read_csv(file_path, header=None, sep=' ')\n",
    "# df\n",
    "# # Normalize the first column (column 0) of the DataFrame\n",
    "# df[0] = (df[0] - df[0].min()) / (df[0].max() - df[0].min())\n",
    "# data = df.iloc[:, 0].values\n",
    "# chosen_system = \"SantaFe\"\n",
    "# dt = 1\n",
    "# T_data = len(data)\n",
    "# data = create_delay_embedding(data, 3)\n",
    "# print(f\"Data length: {T_data}.\")\n",
    "\n",
    "# # Train/Test Split\n",
    "# train_end = 7000\n",
    "# train_input  = data[:train_end]\n",
    "# train_target = data[1:train_end+1]\n",
    "# test_input   = data[train_end:-1]\n",
    "# test_target  = data[train_end+1:]\n",
    "# y_test = test_target\n",
    "# n_test_steps = len(test_target)\n",
    "# time_test = np.arange(n_test_steps) * dt\n",
    "\n",
    "# print(f\"Train size: {len(train_input)}  \\nTest size: {len(test_input)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2434c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizons = [300, 600, 1000]\n",
    "\n",
    "# nrmse_dict = defaultdict(list)\n",
    "# seeds = range(995, 1025)\n",
    "\n",
    "# for seed in seeds:\n",
    "#     lstm_baseline = LSTMBaseline3D(\n",
    "#                         hidden_size=32,         # parameter budget ~ 4800\n",
    "#                         lr=1e-3,\n",
    "#                         epochs=400,\n",
    "#                         device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "#                         seed=seed)\n",
    "#     lstm_baseline.fit(train_input, train_target)\n",
    "\n",
    "#     # one-step roll-out to build an initial vector for auto-regressive mode\n",
    "#     init_vec = train_target[-1]                # last teacher-forced target\n",
    "#     lstm_preds = lstm_baseline.predict_open_loop(test_input)\n",
    "\n",
    "#     nrmse = evaluate_nrmse(lstm_preds, test_target, horizons)\n",
    "#     nrmse_dict['LSTM'].append(nrmse)\n",
    "#     # for horizon, value in nrmse.items():\n",
    "#     #     nrmse_dict[horizon].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed92cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRMSE for Different Prediction Horizons:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "LSTM             \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "300        1.0224924845401087 ± 0.14157896135439874\n",
      "600        0.7022846945297287 ± 0.08675457054383437\n",
      "1000       0.7800291094963395 ± 0.09463118835634546\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "# print(\"-\" * 140)\n",
    "# print(f\"{'LSTM':<17}\")\n",
    "# print(\"-\" * 140)\n",
    "\n",
    "# for horizon in horizons:\n",
    "#     hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['LSTM']]\n",
    "\n",
    "#     print(f\"{horizon:<10}\", end=\" \")\n",
    "#     for vals in [hfr_vals]:\n",
    "#         mean = np.mean(vals)\n",
    "#         std = np.std(vals)\n",
    "#         print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc09bd3",
   "metadata": {},
   "source": [
    "### BIDMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa11860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wfdb\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # ─── Load BIDMC Record ─────────────────────────────────────────────────────\n",
    "# record_id = 'bidmc01'\n",
    "# record = wfdb.rdrecord(record_id, pn_dir='bidmc', sampto=8 * 60 * 125)  # 8 mins at 125Hz\n",
    "# signals = record.p_signal  # shape: (60000, 5)\n",
    "# names = [n.strip().strip(',') for n in record.sig_name]\n",
    "\n",
    "# # ─── Get Indices of ECG Lead II and RESP ──────────────────────────────────\n",
    "# idx_ecg = names.index('II')     # ECG Lead II\n",
    "# idx_resp = names.index('RESP')  # Respiration signal\n",
    "\n",
    "# # ─── Parameters ────────────────────────────────────────────────────────────\n",
    "# N_train = 10000\n",
    "# N_test = 5000\n",
    "# emb_dim = 3\n",
    "\n",
    "# # ─── Select Signals ────────────────────────────────────────────────────────\n",
    "# u = signals[:, idx_ecg]   # input: ECG Lead II\n",
    "# v = signals[:, idx_resp]  # target: RESP\n",
    "\n",
    "# # ─── Normalize to [-1, 1] ──────────────────────────────────────────────────\n",
    "# u_norm = 2 * (u - np.min(u)) / (np.max(u) - np.min(u)) - 1\n",
    "# v_norm = 2 * (v - np.min(v)) / (np.max(v) - np.min(v)) - 1\n",
    "\n",
    "# # ─── Delay Embedding ───────────────────────────────────────────────────────\n",
    "# inputs = create_delay_embedding(u_norm, emb_dim)\n",
    "# targets = create_delay_embedding(v_norm, emb_dim)\n",
    "\n",
    "# # ─── Train/Test Split ──────────────────────────────────────────────────────\n",
    "# train_input = inputs[:N_train]\n",
    "# train_target = targets[:N_train]\n",
    "# test_input = inputs[N_train:N_train+N_test]\n",
    "# test_target = targets[N_train:N_train+N_test]\n",
    "\n",
    "# # ─── Summary ───────────────────────────────────────────────────────────────\n",
    "# print(f\"Train input shape:  {train_input.shape}\")\n",
    "# print(f\"Train target shape: {train_target.shape}\")\n",
    "# print(f\"Test input shape:   {test_input.shape}\")\n",
    "# print(f\"Test target shape:  {test_target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d4c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizons = [300, 600, 1000]\n",
    "\n",
    "# nrmse_dict = defaultdict(list)\n",
    "# seeds = range(995, 1025)\n",
    "\n",
    "# for seed in seeds:\n",
    "#     lstm_baseline = LSTMBaseline3D(\n",
    "#                         hidden_size=32,         # parameter budget ~ 4800\n",
    "#                         lr=1e-3,\n",
    "#                         epochs=400,\n",
    "#                         device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "#                         seed=seed)\n",
    "#     lstm_baseline.fit(train_input, train_target)\n",
    "\n",
    "#     # one-step roll-out to build an initial vector for auto-regressive mode\n",
    "#     init_vec = train_target[-1]                # last teacher-forced target\n",
    "#     lstm_preds = lstm_baseline.predict_open_loop(test_input)\n",
    "\n",
    "#     nrmse = evaluate_nrmse(lstm_preds, test_target, horizons)\n",
    "#     nrmse_dict['LSTM'].append(nrmse)\n",
    "#     # for horizon, value in nrmse.items():\n",
    "#     #     nrmse_dict[horizon].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36703cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "# print(\"-\" * 140)\n",
    "# print(f\"{'LSTM':<17}\")\n",
    "# print(\"-\" * 140)\n",
    "\n",
    "# for horizon in horizons:\n",
    "#     hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['LSTM']]\n",
    "\n",
    "#     print(f\"{horizon:<10}\", end=\" \")\n",
    "#     for vals in [hfr_vals]:\n",
    "#         mean = np.mean(vals)\n",
    "#         std = np.std(vals)\n",
    "#         print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643beaa",
   "metadata": {},
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNBaseline3D(nn.Module):\n",
    "    \"\"\"\n",
    "    2-layer causal TCN       (kernel=3, dilation=1 & 2, padding chosen\n",
    "    so receptive field = 5 time-steps).\n",
    "    ----------------------\n",
    "    • input_dim  = 3\n",
    "    • hidden_dim = 32  → total ≈ 4.9 k parameters\n",
    "    • output_dim = 3    (one-step prediction)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_dim:  int = 3,\n",
    "                 hidden_dim: int = 32,\n",
    "                 output_dim: int = 3,\n",
    "                 lr: float = 1e-3,\n",
    "                 epochs: int = 40,\n",
    "                 device: str = \"cpu\",\n",
    "                 seed: int = 0):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed); np.random.seed(seed)\n",
    "\n",
    "        k = 3  # kernel\n",
    "        # layer 1: dilation 1  → pad 2 to keep length\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim,\n",
    "                               kernel_size=k,\n",
    "                               dilation=1,\n",
    "                               padding=2,\n",
    "                               bias=True)\n",
    "        # layer 2: dilation 2  → pad 4\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim,\n",
    "                               kernel_size=k,\n",
    "                               dilation=2,\n",
    "                               padding=4,\n",
    "                               bias=True)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.head  = nn.Conv1d(hidden_dim, output_dim,\n",
    "                               kernel_size=1, bias=True)\n",
    "\n",
    "        self.lr, self.epochs = lr, epochs\n",
    "        self.to(device)\n",
    "        self.optim = Adam(self.parameters(), lr=lr)\n",
    "        self.crit  = nn.MSELoss()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape  [B, T, 3]  (batch, time, channels)\n",
    "        return  [B, T, 3]\n",
    "        \"\"\"\n",
    "        # reshape to Conv1d convention: (B, C, T)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        y = self.conv1(x); y = self.relu(y[:, :, :-2])     # remove look-ahead pad\n",
    "        y = self.conv2(y); y = self.relu(y[:, :, :-4])     # remove look-ahead pad\n",
    "        out = self.head(y).permute(0, 2, 1)                # back to (B,T,C)\n",
    "        return out\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    def fit(self, x_np: np.ndarray, y_np: np.ndarray):\n",
    "        \"\"\"\n",
    "        Teacher-forcing on entire sequence (batch size = 1).\n",
    "        x_np, y_np shape [T, 3]\n",
    "        \"\"\"\n",
    "        x = torch.tensor(x_np[None], dtype=torch.float32, device=next(self.parameters()).device)\n",
    "        y = torch.tensor(y_np[None], dtype=torch.float32, device=next(self.parameters()).device)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            self.optim.zero_grad()\n",
    "            pred = self.forward(x)\n",
    "            loss = self.crit(pred[:, :-1], y[:, 1:])  # predict next step\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def predict(self, init_window: np.ndarray, n_steps: int):\n",
    "        \"\"\"\n",
    "        Autoregressive roll-out.\n",
    "        init_window : length L≥5, shape [L,3] (latest samples, earliest first)\n",
    "        Returns      : [n_steps,3]\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        window = init_window.copy()\n",
    "        preds  = np.empty((n_steps, 3), dtype=np.float32)\n",
    "\n",
    "        for t in range(n_steps):\n",
    "            inp = torch.tensor(window[None], dtype=torch.float32, device=device)\n",
    "            y   = self.forward(inp)[0, -1].cpu().numpy()\n",
    "            preds[t] = y\n",
    "            window   = np.vstack([window[1:], y])  # slide window\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_open_loop(self, x_np: np.ndarray):\n",
    "        \"\"\"\n",
    "        Open-loop prediction (teacher-forced inputs).\n",
    "        x_np shape: [T, 3]\n",
    "        Returns:\n",
    "            preds: [T - 1, 3] – one-step-ahead predictions\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        x = torch.tensor(x_np[None], dtype=torch.float32, device=device)  # [1, T, 3]\n",
    "        preds = self.forward(x)  # [1, T, 3]\n",
    "        preds = preds[:, :-1]    # predict from t=0 to t=T-2 for target t=1 to t=T-1\n",
    "\n",
    "        return preds.squeeze(0).cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "# tcn = TCNBaseline3D(hidden_dim=32, epochs=50, lr=1e-3, device=\"cpu\", seed=46)\n",
    "# tcn.fit(train_input, train_target)\n",
    "\n",
    "# # initial window must be >4 samples:\n",
    "# init_win = test_input[:5].copy()\n",
    "# tcn_preds = tcn.predict(init_win, n_steps=len(test_target))\n",
    "# tcn_preds_open_loop = tcn.predict_open_loop(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d932f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nrmse = evaluate_nrmse(tcn_preds, y_test, horizons=[200, 400, 600, 800, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34050567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{200: array([3.3622266 , 2.92286399, 2.94476045]),\n",
       " 400: array([3.35475998, 2.8935311 , 2.71048812]),\n",
       " 600: array([3.59476628, 3.17971057, 3.2000023 ]),\n",
       " 800: array([3.6408971 , 3.18923543, 3.18111239]),\n",
       " 1000: array([3.62186296, 3.17139968, 3.19773441])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nrmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdddeff9",
   "metadata": {},
   "source": [
    "### MIT-BIH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9be1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 25000, train size: 15000, test size: 9999\n"
     ]
    }
   ],
   "source": [
    "# MIT-BIH Dataset\n",
    "def create_delay_embedding(signal, embed_dim):\n",
    "    L = len(signal) - embed_dim + 1\n",
    "    emb = np.zeros((L, embed_dim))\n",
    "    for i in range(L):\n",
    "        emb[i, :] = signal[i:i+embed_dim]\n",
    "    return emb\n",
    "\n",
    "import wfdb\n",
    "\n",
    "# Download and load record and annotations for patient #100\n",
    "record = wfdb.rdrecord('100', sampfrom=0, sampto=25002, pn_dir='mitdb')  # first 20,000 samples\n",
    "annotation = wfdb.rdann('100', 'atr', sampfrom=0, sampto=25002, pn_dir='mitdb')\n",
    "# Get input signal u(t) from the first channel\n",
    "u = record.p_signal[:, 0] \n",
    "u\n",
    "# Normalize input\n",
    "u_min = np.min(u)\n",
    "u_max = np.max(u)\n",
    "u_norm = (u - u_min) / (u_max - u_min)\n",
    "fs = record.fs  # sampling frequency (should be 360 Hz)\n",
    "t_vals = np.arange(len(u_norm)) / fs\n",
    "emb_dim = 3\n",
    "# inputs = u_norm\n",
    "inputs = create_delay_embedding(u_norm, emb_dim)\n",
    "\n",
    "# Create target array (heartbeat locations)\n",
    "targets = np.zeros(len(u_norm))\n",
    "targets[annotation.sample] = 1  # mark annotations as 1 (heartbeat)\n",
    "targets = create_delay_embedding(targets, emb_dim)\n",
    "data_size = len(inputs)\n",
    "train_size = 15000\n",
    "train_input = inputs[:train_size]\n",
    "train_target = targets[:train_size]\n",
    "test_input = inputs[train_size+1:]\n",
    "test_target = targets[train_size+1:]\n",
    "test_size = len(test_input)\n",
    "print(f\"Total samples: {data_size}, train size: {train_size}, test size: {test_size}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [300, 600, 1000]\n",
    "\n",
    "nrmse_dict = defaultdict(list)\n",
    "seeds = range(995, 1025)\n",
    "\n",
    "for seed in seeds:  \n",
    "    tcn = TCNBaseline3D(hidden_dim=32, epochs=50, lr=1e-3, device=\"cpu\", seed=seed)\n",
    "    tcn.fit(train_input, train_target)\n",
    "\n",
    "    # initial window must be >4 samples:\n",
    "    # init_win = test_input[:5].copy()    \n",
    "    tcn_preds = tcn.predict_open_loop(test_input)\n",
    "    nrmse = evaluate_nrmse(tcn_preds, test_target, horizons)\n",
    "    nrmse_dict['TCN'].append(nrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36b4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRMSE for Different Prediction Horizons:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "TCN              \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "300        1.620596824802384 ± 0.04332837192484439\n",
      "600        1.6186667098390075 ± 0.04171223835818487\n",
      "1000       1.6169760364489023 ± 0.04015689973379543\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "print(\"-\" * 140)\n",
    "print(f\"{'TCN':<17}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for horizon in horizons:\n",
    "    hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['TCN']]\n",
    "\n",
    "    print(f\"{horizon:<10}\", end=\" \")\n",
    "    for vals in [hfr_vals]:\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783ddd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3315\n",
      "Train size: 2000\n",
      "Test size: 1312\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = 'RealWorld/datasets/SN_m_tot_V2.0.csv'\n",
    "\n",
    "df = pd.read_csv(file_path, sep=';', header = None)\n",
    "df\n",
    "data = df.iloc[:, 3].values\n",
    "dt = 1\n",
    "dataset_size = len(data)\n",
    "data = create_delay_embedding(data, 3)\n",
    "print(f\"Dataset size: {dataset_size}\")\n",
    "\n",
    "# Train/Test Split\n",
    "train_end = 2000\n",
    "train_input  = data[:train_end]\n",
    "train_target = data[1:train_end+1]\n",
    "test_input   = data[train_end:-1]\n",
    "test_target  = data[train_end+1:]\n",
    "y_test = test_target\n",
    "n_test_steps = len(test_target)\n",
    "time_test = np.arange(n_test_steps) * dt\n",
    "\n",
    "print(f\"Train size: {len(train_input)}\\nTest size: {len(test_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a46acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [300, 600, 1000]\n",
    "\n",
    "nrmse_dict = defaultdict(list)\n",
    "seeds = range(995, 1025)\n",
    "\n",
    "for seed in seeds:  \n",
    "    tcn = TCNBaseline3D(hidden_dim=32, epochs=40, lr=1e-3, device=\"cpu\", seed=seed)\n",
    "    tcn.fit(train_input, train_target)\n",
    "\n",
    "    # initial window must be >4 samples:\n",
    "    # init_win = test_input[:5].copy()    \n",
    "    tcn_preds = tcn.predict_open_loop(test_input)\n",
    "    nrmse = evaluate_nrmse(tcn_preds, test_target, horizons)\n",
    "    nrmse_dict['TCN'].append(nrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db20e3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRMSE for Different Prediction Horizons:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "TCN              \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "300        0.920251233826814 ± 0.051167505977109125\n",
      "600        0.7670735717539632 ± 0.05024783059948199\n",
      "1000       0.7532650172790911 ± 0.0496774645230772\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "print(\"-\" * 140)\n",
    "print(f\"{'TCN':<17}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for horizon in horizons:\n",
    "    hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['TCN']]\n",
    "\n",
    "    print(f\"{horizon:<10}\", end=\" \")\n",
    "    for vals in [hfr_vals]:\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a803d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 17000.\n",
      "Train size: 7000  \n",
      "Test size: 9997\n"
     ]
    }
   ],
   "source": [
    "file_path = 'RealWorld/datasets/santa-fe-time-series-competition-data-set-b-1.0.0/b1.txt'\n",
    "\n",
    "df = pd.read_csv(file_path, header=None, sep=' ')\n",
    "df\n",
    "# Normalize the first column (column 0) of the DataFrame\n",
    "df[0] = (df[0] - df[0].min()) / (df[0].max() - df[0].min())\n",
    "data = df.iloc[:, 0].values\n",
    "chosen_system = \"SantaFe\"\n",
    "dt = 1\n",
    "T_data = len(data)\n",
    "data = create_delay_embedding(data, 3)\n",
    "print(f\"Data length: {T_data}.\")\n",
    "\n",
    "# Train/Test Split\n",
    "train_end = 7000\n",
    "train_input  = data[:train_end]\n",
    "train_target = data[1:train_end+1]\n",
    "test_input   = data[train_end:-1]\n",
    "test_target  = data[train_end+1:]\n",
    "y_test = test_target\n",
    "n_test_steps = len(test_target)\n",
    "time_test = np.arange(n_test_steps) * dt\n",
    "\n",
    "print(f\"Train size: {len(train_input)}  \\nTest size: {len(test_input)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [300, 600, 1000]\n",
    "\n",
    "nrmse_dict = defaultdict(list)\n",
    "seeds = range(995, 1025)\n",
    "\n",
    "for seed in seeds:  \n",
    "    tcn = TCNBaseline3D(hidden_dim=32, epochs=100, lr=1e-3, device=\"cpu\", seed=seed)\n",
    "    tcn.fit(train_input, train_target)\n",
    "\n",
    "    # initial window must be >4 samples:\n",
    "    # init_win = test_input[:5].copy()    \n",
    "    tcn_preds = tcn.predict_open_loop(test_input)\n",
    "    nrmse = evaluate_nrmse(tcn_preds, test_target, horizons)\n",
    "    nrmse_dict['TCN'].append(nrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c93c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRMSE for Different Prediction Horizons:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "TCN              \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "300        1.2064525351486612 ± 0.18056236093445113\n",
      "600        0.8929744613975146 ± 0.12518798413065568\n",
      "1000       0.9639359904495735 ± 0.13378513242802026\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "print(\"-\" * 140)\n",
    "print(f\"{'TCN':<17}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for horizon in horizons:\n",
    "    hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['TCN']]\n",
    "\n",
    "    print(f\"{horizon:<10}\", end=\" \")\n",
    "    for vals in [hfr_vals]:\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230bd304",
   "metadata": {},
   "source": [
    "# NVAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e465a0",
   "metadata": {},
   "source": [
    "## MIT-BIH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ebdc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIT-BIH Dataset\n",
    "def create_delay_embedding(signal, embed_dim):\n",
    "    L = len(signal) - embed_dim + 1\n",
    "    emb = np.zeros((L, embed_dim))\n",
    "    for i in range(L):\n",
    "        emb[i, :] = signal[i:i+embed_dim]\n",
    "    return emb\n",
    "\n",
    "import wfdb\n",
    "\n",
    "# Download and load record and annotations for patient #100\n",
    "record = wfdb.rdrecord('100', sampfrom=0, sampto=25002, pn_dir='mitdb')  # first 20,000 samples\n",
    "annotation = wfdb.rdann('100', 'atr', sampfrom=0, sampto=25002, pn_dir='mitdb')\n",
    "# Get input signal u(t) from the first channel\n",
    "u = record.p_signal[:, 0] \n",
    "u\n",
    "# Normalize input\n",
    "u_min = np.min(u)\n",
    "u_max = np.max(u)\n",
    "u_norm = (u - u_min) / (u_max - u_min)\n",
    "fs = record.fs  # sampling frequency (should be 360 Hz)\n",
    "t_vals = np.arange(len(u_norm)) / fs\n",
    "emb_dim = 3\n",
    "# inputs = u_norm\n",
    "inputs = create_delay_embedding(u_norm, emb_dim)\n",
    "\n",
    "# Create target array (heartbeat locations)\n",
    "targets = np.zeros(len(u_norm))\n",
    "targets[annotation.sample] = 1  # mark annotations as 1 (heartbeat)\n",
    "targets = create_delay_embedding(targets, emb_dim)\n",
    "data_size = len(inputs)\n",
    "train_size = 15000\n",
    "train_input = inputs[:train_size]\n",
    "train_target = targets[:train_size]\n",
    "test_input = inputs[train_size+1:]\n",
    "test_target = targets[train_size+1:]\n",
    "test_size = len(test_input)\n",
    "print(f\"Total samples: {data_size}, train size: {train_size}, test size: {test_size}\") \n",
    "horizons = [300, 600, 1000]\n",
    "\n",
    "nrmse_dict = defaultdict(list)\n",
    "seeds = range(995, 1025)\n",
    "\n",
    "for seed in seeds:  \n",
    "    tcn = TCNBaseline3D(hidden_dim=32, epochs=50, lr=1e-3, device=\"cpu\", seed=seed)\n",
    "    tcn.fit(train_input, train_target)\n",
    "\n",
    "    # initial window must be >4 samples:\n",
    "    # init_win = test_input[:5].copy()    \n",
    "    tcn_preds = tcn.predict_open_loop(test_input)\n",
    "    nrmse = evaluate_nrmse(tcn_preds, test_target, horizons)\n",
    "    nrmse_dict['TCN'].append(nrmse)\n",
    "\n",
    "print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "print(\"-\" * 140)\n",
    "print(f\"{'TCN':<17}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for horizon in horizons:\n",
    "    hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['TCN']]\n",
    "\n",
    "    print(f\"{horizon:<10}\", end=\" \")\n",
    "    for vals in [hfr_vals]:\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef905e69",
   "metadata": {},
   "source": [
    "## BIDMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334adf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ─── Load BIDMC Record ─────────────────────────────────────────────────────\n",
    "record_id = 'bidmc01'\n",
    "record = wfdb.rdrecord(record_id, pn_dir='bidmc', sampto=8 * 60 * 125)  # 8 mins at 125Hz\n",
    "signals = record.p_signal  # shape: (60000, 5)\n",
    "names = [n.strip().strip(',') for n in record.sig_name]\n",
    "\n",
    "# ─── Get Indices of ECG Lead II and RESP ──────────────────────────────────\n",
    "idx_ecg = names.index('II')     # ECG Lead II\n",
    "idx_resp = names.index('RESP')  # Respiration signal\n",
    "\n",
    "# ─── Parameters ────────────────────────────────────────────────────────────\n",
    "N_train = 10000\n",
    "N_test = 5000\n",
    "emb_dim = 3\n",
    "\n",
    "# ─── Select Signals ────────────────────────────────────────────────────────\n",
    "u = signals[:, idx_ecg]   # input: ECG Lead II\n",
    "v = signals[:, idx_resp]  # target: RESP\n",
    "\n",
    "# ─── Normalize to [-1, 1] ──────────────────────────────────────────────────\n",
    "u_norm = 2 * (u - np.min(u)) / (np.max(u) - np.min(u)) - 1\n",
    "v_norm = 2 * (v - np.min(v)) / (np.max(v) - np.min(v)) - 1\n",
    "\n",
    "# ─── Delay Embedding ───────────────────────────────────────────────────────\n",
    "inputs = create_delay_embedding(u_norm, emb_dim)\n",
    "targets = create_delay_embedding(v_norm, emb_dim)\n",
    "\n",
    "# ─── Train/Test Split ──────────────────────────────────────────────────────\n",
    "train_input = inputs[:N_train]\n",
    "train_target = targets[:N_train]\n",
    "test_input = inputs[N_train:N_train+N_test]\n",
    "test_target = targets[N_train:N_train+N_test]\n",
    "\n",
    "# ─── Summary ───────────────────────────────────────────────────────────────\n",
    "print(f\"Train input shape:  {train_input.shape}\")\n",
    "print(f\"Train target shape: {train_target.shape}\")\n",
    "print(f\"Test input shape:   {test_input.shape}\")\n",
    "print(f\"Test target shape:  {test_target.shape}\")\n",
    "horizons = [300, 600, 1000]\n",
    "\n",
    "nrmse_dict = defaultdict(list)\n",
    "seeds = range(995, 1025)\n",
    "\n",
    "for seed in seeds:\n",
    "    nvar_baseline = NVARBaseline3D(\n",
    "                        hidden_size=32,         # parameter budget ~ 4800\n",
    "                        lr=1e-3,\n",
    "                        epochs=400,\n",
    "                        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                        seed=seed)\n",
    "    nvar_baseline.fit(train_input, train_target)\n",
    "\n",
    "    # one-step roll-out to build an initial vector for auto-regressive mode\n",
    "    init_vec = train_target[-1]                # last teacher-forced target\n",
    "    lstm_preds = nvar_baseline.predict_open_loop(test_input)\n",
    "\n",
    "    nrmse = evaluate_nrmse(lstm_preds, test_target, horizons)\n",
    "    nrmse_dict['NVAR'].append(nrmse)\n",
    "    # for horizon, value in nrmse.items():\n",
    "    #     nrmse_dict[horizon].append(value)\n",
    "\n",
    "print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "print(\"-\" * 140)\n",
    "print(f\"{'LSTM':<17}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for horizon in horizons:\n",
    "    hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['NVAR']]\n",
    "\n",
    "    print(f\"{horizon:<10}\", end=\" \")\n",
    "    for vals in [hfr_vals]:\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58809d1",
   "metadata": {},
   "source": [
    "### Canonical Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f4662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorenz_deriv(state, t, sigma=10.0, rho=28.0, beta=8.0/3.0):\n",
    "    x, y, z = state\n",
    "    dxdt = sigma * (y - x)\n",
    "    dydt = x*(rho - z) - y\n",
    "    dzdt = x*y - beta*z\n",
    "    return [dxdt, dydt, dzdt]\n",
    "\n",
    "def generate_lorenz_data(\n",
    "    initial_state=[1.0, 1.0, 1.0],\n",
    "    tmax=25.0,\n",
    "    dt=0.01,\n",
    "    sigma=10.0,\n",
    "    rho=28.0,\n",
    "    beta=8.0/3.0\n",
    "):\n",
    "    num_steps = int(tmax / dt) + 1 # +1 to include t=0\n",
    "    t_vals = np.linspace(0, tmax, num_steps)\n",
    "    sol = odeint(lorenz_deriv, initial_state, t_vals, args=(sigma, rho, beta))\n",
    "    return t_vals, sol\n",
    "\n",
    "def rossler_derivatives(state, t, a=0.2, b=0.2, c=5.7):\n",
    "    \"\"\"Compute time derivatives [dx/dt, dy/dt, dz/dt] for the Rössler system.\"\"\"\n",
    "    x, y, z = state\n",
    "    dxdt = -y - z\n",
    "    dydt = x + a * y\n",
    "    dzdt = b + z * (x - c)\n",
    "    return [dxdt, dydt, dzdt]\n",
    "\n",
    "def generate_rossler_data(\n",
    "    initial_state=[1.0, 0.0, 0.0],\n",
    "    tmax=25.0,\n",
    "    dt=0.01,\n",
    "    a=0.2,\n",
    "    b=0.2,\n",
    "    c=5.7\n",
    "):\n",
    "    \"\"\"\n",
    "    Numerically integrate Rössler equations x'(t), y'(t), z'(t) using odeint.\n",
    "    Returns:\n",
    "       t_vals: array of time points\n",
    "       sol   : array shape [num_steps, 3] of [x(t), y(t), z(t)]\n",
    "    \"\"\"\n",
    "    num_steps = int(tmax / dt)\n",
    "    t_vals = np.linspace(0, tmax, num_steps)\n",
    "    sol = odeint(rossler_derivatives, initial_state, t_vals, args=(a, b, c))\n",
    "    return t_vals, sol\n",
    "\n",
    "def chen_deriv(state, t, a=35.0, b=3.0, c=28.0):\n",
    "    \"\"\"\n",
    "    Computes derivatives [dx/dt, dy/dt, dz/dt] for Chen system:\n",
    "      dx/dt = a*(y - x)\n",
    "      dy/dt = (c - a)*x + c*y - x*z\n",
    "      dz/dt = x*y - b*z\n",
    "    \"\"\"\n",
    "    x, y, z = state\n",
    "    dxdt = a*(y - x)\n",
    "    dydt = (c - a)*x + c*y - x*z\n",
    "    dzdt = x*y - b*z\n",
    "    return [dxdt, dydt, dzdt]\n",
    "\n",
    "def generate_chen_data(\n",
    "    initial_state=[1.0, 1.0, 1.0],\n",
    "    tmax=50.0,\n",
    "    dt=0.01,\n",
    "    a=35.0,\n",
    "    b=3.0,\n",
    "    c=28.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Integrates Chen's system from 'initial_state' up to time 'tmax' with step size 'dt'.\n",
    "    Returns:\n",
    "      t_vals: time array of length T\n",
    "      sol   : array shape [T, 3], the trajectory [x(t), y(t), z(t)]\n",
    "    \"\"\"\n",
    "    num_steps = int(tmax / dt)\n",
    "    t_vals = np.linspace(0, tmax, num_steps)\n",
    "    sol = odeint(chen_deriv, initial_state, t_vals, args=(a, b, c))\n",
    "    return t_vals, sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8089a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"input_dim\": [3],\n",
    "    \"hidden_size\": [37],\n",
    "    \"output_dim\": [3],\n",
    "    \"lr\": [1e-3],\n",
    "    \"epochs\": [30],\n",
    "    \"device\": ['cuda'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b326ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(model_class, param_grid, model_name,\n",
    "                    output_path=\"grid_search_results.json\", f=generate_chen_data, lambda_max=0.9):\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    param_keys = list(param_grid.keys())\n",
    "    print(f\"\\n== Initial grid search for {model_name} with {len(combos)} combinations ==\")\n",
    "\n",
    "    results = []\n",
    "    horizons = list(range(10, 1001, 10))\n",
    "    # horizons = [200, 400, 600, 800, 1000]\n",
    "    \n",
    "\n",
    "    for comb in tqdm(combos, desc=\"Grid Search\"):\n",
    "        params = dict(zip(param_keys, comb))\n",
    "        seed_scores_vpt = []\n",
    "        horizon_nrmse_all = {h: [] for h in horizons}\n",
    "        adev_scores = []\n",
    "        # ldev_scores = []\n",
    "\n",
    "        for initial_state in [[1.0, 1.0, 1.0], [1.0, 2.0, 3.0], [2.0, 1.5, 4.0]]:\n",
    "            tmax = 250\n",
    "            dt = 0.02\n",
    "            t_vals, lorenz_traj = f(\n",
    "                initial_state=initial_state,\n",
    "                tmax=tmax,\n",
    "                dt=dt\n",
    "            )\n",
    "\n",
    "            washout = 2000\n",
    "            t_vals = t_vals[washout:]\n",
    "            lorenz_traj = lorenz_traj[washout:]\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(lorenz_traj)\n",
    "            lorenz_traj = scaler.transform(lorenz_traj)\n",
    "\n",
    "            T_data = len(lorenz_traj)\n",
    "            for train_frac in [0.7, 0.75, 0.8]:\n",
    "                train_end = int(train_frac * (T_data - 1))\n",
    "                train_input = lorenz_traj[:train_end]\n",
    "                train_target = lorenz_traj[1:train_end + 1]\n",
    "                test_input = lorenz_traj[train_end:-1]\n",
    "                test_target = lorenz_traj[train_end + 1:]\n",
    "                n_test_steps = len(test_input)\n",
    "                initial_in = test_input[0]\n",
    "\n",
    "                for seed in np.arange(1, 5):\n",
    "                    model = model_class(**params, seed=seed)\n",
    "                    model.fit_readout(train_input, train_target, discard=100)\n",
    "                    preds = model.predict_autoregressive(initial_in, n_test_steps)\n",
    "\n",
    "                    T_VPT_s, _, ratio = compute_valid_prediction_time(test_target, preds, t_vals, 0.4, lambda_max, dt)\n",
    "                    seed_scores_vpt.append(ratio)\n",
    "\n",
    "                    horizon_nrmse = evaluate_nrmse(preds, test_target, horizons)\n",
    "                    for h in horizons:\n",
    "                        horizon_nrmse_all[h].append(horizon_nrmse[h])\n",
    "\n",
    "                    adev = compute_attractor_deviation(preds, test_target)\n",
    "                    adev_scores.append(adev)\n",
    "\n",
    "                    # ldev = compute_lyapunov_exponent(\"Lorenz\", preds, dt)\n",
    "                    # ldev_scores.append(ldev)\n",
    "\n",
    "        mean_vpt = float(np.mean(seed_scores_vpt))\n",
    "        std_vpt = float(np.std(seed_scores_vpt))\n",
    "        mean_nrmse_dict = {str(h): float(np.mean(horizon_nrmse_all[h])) for h in horizons}\n",
    "        std_nrmse_dict  = {str(h): float(np.std(horizon_nrmse_all[h]))  for h in horizons}\n",
    "        mean_adev = float(np.mean(adev_scores))\n",
    "        std_adev = float(np.std(adev_scores))\n",
    "        # mean_ldev = float(np.mean(ldev_scores))\n",
    "        # std_ldev = float(np.std(ldev_scores))\n",
    "\n",
    "        results.append({\n",
    "            \"params\": params,\n",
    "            \"seed_scores_T_VPT\": seed_scores_vpt,\n",
    "            \"mean_T_VPT\": mean_vpt,\n",
    "            \"std_T_VPT\": std_vpt,\n",
    "            # \"mean_NRMSEs\": mean_nrmse_dict,\n",
    "            # \"std_NRMSEs\": std_nrmse_dict,\n",
    "            \"mean_ADev\": mean_adev,\n",
    "            \"std_ADev\": std_adev,\n",
    "            # \"mean_LDev\": mean_ldev,\n",
    "            # \"std_LDev\": std_ldev\n",
    "        })\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nAll results saved to `{output_path}`\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569374f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_grid_search(LSTMBaseline3D, grid, \"lstm\", output_path=\"lstm.json\", f=generate_lorenz_data, lambda_max=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
